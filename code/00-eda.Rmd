
---
title: "Gray Whale EDA"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries & data

```{r}

# libraries
library(dplyr)
library(tidyverse)
library(vegan)
library(ggplot2)

# dataset. Note you'll need to change the file path to match where it is on your computer
#data <- read.csv("Users/cmantegna/Documents/GitHub/sicb_dp/data/graywhales.csv")

```

# Data cleaning
## uniform variables and backfilling the empty cells
```{r}

# fix variable names in the channel and source columns. 
# since both uppercase and lowercase are used, they will be treated as unique cases, this step stops that from happening. The 'dplyr' package does this.
data <- data %>%
  mutate(Channel = tolower(Channel),
         Source = tolower(Source))

# fill in missing values
# after reviewing the missing data, the missing times were filled in manually with 0:00. 
# missing counts were replaced with 1 because at least one whale was seen to report it.
# missing source or channel names were replaced with 'unknown'
data <- data %>%
  mutate(Count = ifelse(is.na(Count), 1, Count),
         Source = ifelse(is.na(Source), "unknown", Source))

# write out cleaned data to work with later. this is a safety stop so you don't have to keep doing it. Make sure you update your path to the correct one on your computer.
#write_csv(data, "Users/cmantegna/Documents/GitHub/sicb_dp/data/cleaned_data.csv")

```

## pull out the month from the sighting date
```{r}

# convert SightDate to date format and extract the month
data <- data %>%
  mutate(SightDate = as.Date(SightDate, format = "%m/%d/%y"),
         Month = format(SightDate, "%m"))
         
```

# EDA
## Counts, total

```{r}

# total number of whales sighted from 2013 - 2023
# save the table it creates if you don't want to copy paste the data OR you can knit this markdown file to a pdf or html file that supports printing/ manipulation.

total_count <- sum(data$Count, na.rm = TRUE)
print(paste("Total Whale Count:", total_count))

```


## Counts, monthly
```{r}

# total number of whales counted per month regardless of year

monthly_counts <- data %>%
  group_by(Month) %>%
  summarise(TotalCount = sum(Count, na.rm = TRUE))

print(monthly_counts)

```

## Counts, seasonally
```{r}

# create a season column
data <- data %>%
  mutate(Season = case_when(
    month(SightDate) %in% c(12, 1, 2) ~ "Winter",
    month(SightDate) %in% c(3, 4, 5) ~ "Spring",
    month(SightDate) %in% c(6, 7, 8) ~ "Summer",
    month(SightDate) %in% c(9, 10, 11) ~ "Fall"
  ))

# group by season
seasonal_counts <- data %>%
  group_by(Season) %>%
  summarise(TotalCount = sum(Count, na.rm = TRUE)) %>%
  arrange(match(Season, c("Winter", "Spring", "Summer", "Fall")))

# print
print(seasonal_counts)

# plot seasonal trends
library(ggplot2)

ggplot(seasonal_counts, aes(x = Season, y = TotalCount, fill = Season)) +
  geom_bar(stat = "identity") +
  labs(title = "Whale Sightings by Season",
       x = "Season",
       y = "Total Count of Sightings") +
  theme_minimal()

```


## Counts, quad

```{r}

# total number of whales counted per quad regardless of year
quad_counts <- data %>%
  group_by(Quad) %>%
  summarise(TotalCount = sum(Count, na.rm = TRUE))

print(quad_counts)

```

# Channel and Source Summaries
## double check this is an accurate way to look at this. would work as a 'source' pie chart.
```{r}

# need to go back and review if I set this up correctly - this is treating these like independent variables, not dependent...

channel_summary <- data %>%
  count(Channel)

source_summary <- data %>%
  count(Source)

print(channel_summary)
print(source_summary)

```

# Spatial Analysis
## Temporal
## very, very busy - will need to plot per year, maybe animate
```{r}

# libraries
library(ggplot2)
library(dplyr)
library(lubridate)
library(sf) # For spatial data handling
library(ggspatial) # For map enhancements

# group data by season - we did this in code lines 88 - 95, so no need to do again unless you move that code chunk below this one (we can go back and break down spring by month since it is the overwhelming majority of sightings across the years).

# group data for spatial plotting
spatial_data <- data %>%
  group_by(Lat, Long, Season) %>%
  summarise(TotalCount = sum(Count, na.rm = TRUE))

# bounding box for Washington State and San Juan Islands
washington_bbox <- c(xmin = -125, xmax = -120, ymin = 46, ymax = 49)

# load a world map and crop to bounding box
world <- st_as_sf(maps::map("world", plot = FALSE, fill = TRUE))

# Plot whale migration patterns focusing on Washington State
ggplot() +
  geom_sf(data = world, fill = "gray80", color = "white") + # Base map
  geom_point(data = spatial_data, aes(x = Long, y = Lat, size = TotalCount, color = Season), alpha = 0.7) +
  scale_color_manual(values = c("Winter" = "blue", "Spring" = "green", "Summer" = "yellow", "Fall" = "orange")) +
  labs(title = "Seasonal Whale Migration Patterns in Washington State",
       x = "Longitude",
       y = "Latitude",
       size = "Whale Count",
       color = "Season") +
  coord_sf(xlim = c(washington_bbox["xmin"], washington_bbox["xmax"]),
           ylim = c(washington_bbox["ymin"], washington_bbox["ymax"])) + # Crop to Washington
  theme_minimal() +
  theme(legend.position = "right") +
  annotation_scale(location = "bl", width_hint = 0.5) +
  annotation_north_arrow(location = "tl", which_north = "true", style = north_arrow_fancy_orienteering())

```

## Time lapse - this is still incorrect - DON'T run until troubleshooting is complete
```{r}

#install.packages("gganimate")
library("gganimate")

# Add a Month-Year column for animation
data <- data %>%
  mutate(MonthYear = format(SightDate, "%Y-%m"))

# Plot and animate
ggplot(data, aes(x = "Long", y = "Lat", size = "Count", color = "Season")) +
  geom_point(alpha = 0.6) +
  geom_sf(data = world, fill = "gray80", color = "white") +
  transition_time(as.Date(SightDate)) +
  labs(title = "Whale Sightings: {frame_time}",
       x = "Longitude", y = "Latitude") +
  theme_minimal()

```

## group size trends
### super busy, may want to break down by year
```{r}

# libraries
library(dplyr)
library(ggplot2)

# group size trends over time
group_size_trends <- data %>%
  group_by(Year, Month) %>%
  summarise(AverageGroupSize = mean(Count, na.rm = TRUE)) %>%
  arrange(Year, Month)

print(group_size_trends)

# plot group size trends
ggplot(group_size_trends, aes(x = interaction(Year, Month, sep = "-"), y = AverageGroupSize, group = 1)) +
  geom_line() +
  geom_point() +
  labs(title = "Average Whale Group Size Over Time",
       x = "Year-Month",
       y = "Average Group Size") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_line(color = "gray90"))

```

## more group trends
### fix year to be a character + separate quad to per year/ season anything smaller
```{r}

# libraries
library(dplyr)
library(ggplot2)


# trends over time (yearly and monthly)
group_size_yearly <- data %>%
  group_by(Year) %>%
  summarise(AverageGroupSize = mean(Count, na.rm = TRUE),
            MaxGroupSize = max(Count, na.rm = TRUE),
            MinGroupSize = min(Count, na.rm = TRUE))

group_size_monthly <- data %>%
  group_by(Year, Month) %>%
  summarise(AverageGroupSize = mean(Count, na.rm = TRUE))

# plot yearly trends
ggplot(group_size_yearly, aes(x = Year, y = AverageGroupSize, group = 1)) +
  geom_line() +
  geom_point() +
  labs(title = "Yearly Average Group Size",
       x = "Year",
       y = "Average Group Size") +
  theme_minimal()

# group size distribution
ggplot(data, aes(x = Count)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Group Sizes",
       x = "Group Size",
       y = "Frequency") +
  theme_minimal()

# group size by location (e.g., Quad)
group_size_by_location <- data %>%
  group_by(Quad) %>%
  summarise(AverageGroupSize = mean(Count, na.rm = TRUE),
            MaxGroupSize = max(Count, na.rm = TRUE),
            MinGroupSize = min(Count, na.rm = TRUE))

# plot average group size by location
ggplot(group_size_by_location, aes(x = as.factor(Quad), y = AverageGroupSize)) +
  geom_bar(stat = "identity", fill = "lightblue", color = "black") +
  labs(title = "Average Group Size by Location (Quad)",
       x = "Quad",
       y = "Average Group Size") +
  theme_minimal()


```

# Sighting probability model

## troubleshooting code errors in adding dates (not necessary to rerun, but i left the code in so that if it happens again i know what to check)
```{r}

str(data) #check sightdate is actually formatted as a date
names(data) #check sightdate is a column and not accidentally made a function based on earlier manipulations
head(data$SightDate) #check what it looks like in the df by itself

```

## add dates without sightings (full code from chatgpt code)
```{r}

# Create a sequence of all possible dates within the dataset's range
#all_dates <- seq(min(data$SightDate), max(data$SightDate), by = "day")
all_dates <- seq(as.Date("2013-01-01"), as.Date("2023-12-31"), by = "day")

# Create a data frame of "no sighting" days
no_sighting_data <- data.frame(SightDate = all_dates) %>%
  anti_join(data, by = "SightDate") %>%
  mutate(Sighting = 0, Month = format(SightDate, "%m"))

# Combine with existing data
data <- data %>%
  mutate(Sighting = 1) %>% # Add a Sighting column to existing data
  bind_rows(no_sighting_data)

```

## fixing new NA values based on the addition of dates with no sightings (chatgpt code)
```{r}

# Function to determine season based on the month
get_season <- function(month) {
  case_when(
    month %in% c(12, 1, 2) ~ "Winter",
    month %in% c(3, 4, 5) ~ "Spring",
    month %in% c(6, 7, 8) ~ "Summer",
    month %in% c(9, 10, 11) ~ "Fall"
  )
}

# Fill NA values for "no sighting" rows
test_data <- data %>%
  mutate(
    Year = ifelse(is.na(Year), format(SightDate, "%Y"), Year),   # Use year from SightDate
    Time = ifelse(is.na(Time), "9:99", Time),                   # Default time
    FishArea = ifelse(is.na(FishArea), "none", FishArea),       # Placeholder for Fish Area
    UTMx = ifelse(is.na(UTMx), 0, UTMx),                       # Default UTMx
    UTMy = ifelse(is.na(UTMy), 0, UTMy),                       # Default UTMy
    Count = ifelse(is.na(Count), 0, Count),                    # Default count
    Channel = ifelse(is.na(Channel), "none", Channel),         # Placeholder for Channel
    Source = ifelse(is.na(Source), "none", Source),            # Placeholder for Source
    Quad = ifelse(is.na(Quad), "none", Quad),               # Placeholder for Quad
    Lat = ifelse(is.na(Lat), 0, Lat),                          # Default Latitude
    Long = ifelse(is.na(Long), 0, Long),                       # Default Longitude
    Season = ifelse(is.na(Season), get_season(as.numeric(Month)), Season) # Assign correct season
  )

```


## note shift to using test_data for this next step. this model does not converge - moved through trouble shooting and attempting to model by year to see if any specific year is throwing us off.
## model 1 of
```{r}

# libraries
#install.packages("caret")
library(dplyr)
library(ggplot2)
library(caret) # For modeling
library(MASS)  # For stepwise regression

# USE THIS SELECT code - it explicitly calls on dplyr to perform the command
# troubleshooting the next code bit (selecting predictors)
model_data <- test_data %>%
  dplyr::select(Sighting, Month, Quad, Count) %>%
  mutate(Month = as.numeric(Month)) # Convert to numeric for modeling


# select relevant predictors - OG code
#model_data <- test_data %>%
#  select(Sighting, Month, Quad, Lat, Long, Count) %>%
#  mutate(Month = as.numeric(Month)) # Convert to numeric for modeling

# split into training and testing sets
set.seed(123)
train_index <- createDataPartition(model_data$Sighting, p = 0.7, list = FALSE)
train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

# Logistic regression model for sighting probability
# didn't converge, so there is something underlying going on that we will address in the next code block
sighting_model <- glm(Sighting ~ Month + Quad, 
                      data = train_data, 
                      family = binomial(link = "logit"))

summary(sighting_model)

# Predict on test data
test_data$PredictedProb <- predict(sighting_model, newdata = test_data, type = "response")
test_data$PredictedClass <- ifelse(test_data$PredictedProb > 0.5, 1, 0)

# Evaluate model performance
confusion_matrix <- confusionMatrix(as.factor(test_data$PredictedClass), as.factor(test_data$Sighting))
print(confusion_matrix)

```

## troubleshooting the non-convergence of the model - DON'T RUN
```{r}

#multi-colinearity?
cor(test_data %>% dplyr::select(Lat, Long, Count)) #looks like we can remove lat/ long

#scale predictors? i don't understand this completely, just following the chatgpt steps for troubleshooting
test_data <- test_data %>%
  mutate(
    Lat = scale(Lat),
    Long = scale(Long),
    Count = scale(Count)
  )

#balancing? From ChatGPT: If the dataset has a significant class imbalance, downsample the majority class or upsample the minority class
library(dplyr)
library(tidyr)

# Downsample majority class (Sighting = 0)
balanced_data <- test_data %>%
  group_by(Sighting) %>%
  sample_n(min(table(test_data$Sighting)))





```

## model 2 of
## different model to try - also does not converge and i will need to use chatgpt to help me understand the output
```{r}

#different number of training iterations
sighting_model <- glm(Sighting ~ Month + Quad + Count, 
                      data = train_data, 
                      family = binomial(link = "logit"),
                      control = glm.control(maxit = 50)) # Default is 25


library(glmnet)

# Prepare data for glmnet (x = predictors, y = response)
x <- model.matrix(Sighting ~ Month + Quad + Count, data = train_data)[, -1]
y <- train_data$Sighting

# Fit a penalized logistic regression model (lasso)
lasso_model <- cv.glmnet(x, y, family = "binomial", alpha = 1)

```

## model 3 of
## trying year by year step through (2023 DNC, )
```{r}

# filter the data for a single year
single_year_data <- test_data %>%
  filter(format(SightDate, "%Y") == "2023")

# check balance of Sighting
table(single_year_data$Sighting)

#check summary of predictors
summary(single_year_data)

# Split into training and testing sets
set.seed(123)
train_index <- createDataPartition(single_year_data$Sighting, p = 0.7, list = FALSE)
train_data <- single_year_data[train_index, ]
test_data <- single_year_data[-train_index, ]

# Train logistic regression
single_year_model <- glm(Sighting ~ Month + Quad + Count, 
                         data = train_data, 
                         family = binomial(link = "logit"))

# Summarize the model
summary(single_year_model)

# Predict on test data
test_data$PredictedProb <- predict(single_year_model, newdata = test_data, type = "response")
test_data$PredictedClass <- ifelse(test_data$PredictedProb > 0.5, 1, 0)

# Evaluate performance
library(caret)
confusion_matrix <- confusionMatrix(as.factor(test_data$PredictedClass), as.factor(test_data$Sighting))
print(confusion_matrix)


```

